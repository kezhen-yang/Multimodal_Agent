{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4edbe397",
      "metadata": {
        "id": "4edbe397"
      },
      "source": [
        "# **Build a Multi-Modal Generation Agent**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44fe0ead",
      "metadata": {
        "id": "44fe0ead"
      },
      "source": [
        "In this project, I'll use open-source text-to-image and text-to-video models to generate content. Next, I'll build a **unified multi-modal agent** similar to modern chatbots, where a single agent can support general questions, image generation, and video generation requests.\n",
        "\n",
        "This project show how to integrate multiple model types under one routing system capable of deciding what modality to use based on the user's intent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b842e61",
      "metadata": {
        "id": "8b842e61"
      },
      "source": [
        "* Use **Text-to-Image** models to generate images from a text.\n",
        "* Generate short clips with a **Text-to-Video** model\n",
        "* Build a **Multi-Modal Agent** that answers questions and routes media requests\n",
        "* Build a simple **Gradio** UI and interact with the multi-modal agent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd9ad71d",
      "metadata": {
        "id": "fd9ad71d"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "In this project, I'll use open-source Text-to-Image and Text-to-Video models to generate visuals from natural-language prompts. These models are computationally heavy and perform best on GPUs, so its recommended to run this notebook in Google Colab or another GPU-enabled environment. I'll load all models from Hugging Face, which requires authentication.\n",
        "\n",
        "Before continuing:\n",
        "\n",
        "Create a Hugging Face account and generate an access token at huggingface.co/settings/tokens\n",
        "\n",
        "Paste your token in the field below to log in.\n",
        "\n",
        "In the Colab environment, enable GPU acceleration by selecting Runtime â†’ Change runtime type â†’ GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yNp2NUkF8p3m",
      "metadata": {
        "id": "yNp2NUkF8p3m"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"Colab Token\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6da7ce1",
      "metadata": {
        "id": "b6da7ce1"
      },
      "source": [
        "Let's import the required libraries and confirm that PyTorch can detect the available GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7950f70c",
      "metadata": {
        "id": "7950f70c"
      },
      "outputs": [],
      "source": [
        "import torch, diffusers, transformers, os, random, gc\n",
        "print('torch', torch.__version__, '| CUDA:', torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer a library by huggingFace allow you to load pre-trained LLMs\n",
        "# diffusers is equivalent of transformer by huggingface for image and video under huggingFace"
      ],
      "metadata": {
        "id": "X-xRUii1OBCj"
      },
      "id": "X-xRUii1OBCj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "028f7c15",
      "metadata": {
        "id": "028f7c15"
      },
      "source": [
        "## Text-to-Image (T2I)\n",
        "T2I models translate natural-language descriptions into images. They are typically based on diffusion models, which gradually refine random noise into a coherent picture guided by the text prompt. In this section, I'll load and test one such model to generate images directly from text inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cfbedd4",
      "metadata": {
        "id": "8cfbedd4"
      },
      "outputs": [],
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "# Define the Stable Diffusion XL model ID from Hugging Face and load the pre-trained model\n",
        "# model_id =  \"stabilityai/stable-diffusion-xl-base-1.0\" # this is a larger model\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\" # this is a smaller model\n",
        "\n",
        "# torch.cuda.empty_cache() # Clear any cached memory before loading a new model\n",
        "\n",
        "image_pipeline = DiffusionPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    dtype=torch.float16, # this is efficient, but can also run with float32 but its going to be large\n",
        "    variant='fp16'\n",
        ").to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# when the sequence length is small, the overhead is more\n",
        "# when the sequence length is large, for example 32,000 or beyong, then thats where the real benefit would appear\n",
        "# you gonna save a lot on memory\n",
        "\n",
        "image_pipeline.enable_attention_slicing()\n",
        "image_pipeline.enable_model_cpu_offload()\n",
        "\n",
        "# Purpose: reduces GPU memory (VRAM) usage during the pipeline's attention computations by splitting attention into smaller slices\n",
        "# instead of computing the whole attention matrix at once.\n",
        "\n",
        "# When to use: run large diffusion models (e.g., SDXL) on limitedâ€‘memory GPUs to avoid OOM errors.\n",
        "\n",
        "# Tradeoffs: lowers peak memory but can slightly increase inference time.\n",
        "\n",
        "# Typical placement: call once after loading the DiffusionPipeline (as in your notebook) before generating images."
      ],
      "metadata": {
        "id": "HDxvWVj6iaqy"
      },
      "id": "HDxvWVj6iaqy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "090367b8",
      "metadata": {
        "id": "090367b8"
      },
      "source": [
        "## Text-to-Video (T2V)\n",
        "T2V models extend the idea of diffusion from still images to moving sequences. Instead of generating one frame, they create a series of coherent frames that depict motion consistent with the text prompt. These models are computationally heavier and often generate short clips (typically 2-10 seconds).\n",
        "\n",
        "In this section, I'll load an open-source video diffusion model and prepare it for generation.\n",
        "\n",
        "### Load a T2V model\n",
        "\n",
        "We'll use the model `damo-vilab/text-to-video-ms-1.7b`, which can produce short video clips from text prompts. This model benefits from a specialized scheduler (DPMSolverMultistepScheduler) that improves stability and speed during sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e261bcd3",
      "metadata": {
        "id": "e261bcd3"
      },
      "outputs": [],
      "source": [
        "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
        "\n",
        "video_model_id = 'damo-vilab/text-to-video-ms-1.7b'\n",
        "\n",
        "# Load the model with FP16 precision for efficiency\n",
        "video_pipeline = DiffusionPipeline.from_pretrained(\n",
        "    video_model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    variant=\"fp16\"\n",
        ")\n",
        "video_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(\n",
        "    video_pipeline.scheduler.config,\n",
        "    algorithm_type=\"sde-dpmsolver++\"\n",
        ")\n",
        "video_pipeline.enable_model_cpu_offload()\n",
        "# video_pipeline.enable_attention_slicing()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3aac4db",
      "metadata": {
        "id": "d3aac4db"
      },
      "source": [
        "## Multimodal Generation Agent\n",
        "Now that I have text-to-image, text-to-video, and basic LLM question answering, I will build a single agent that routes user requests to the right capability. The agent will read a prompt, infer intent (chat vs image vs video), and return the appropriate output.\n",
        "\n",
        "### Load an LLM for generic queries\n",
        "Use a small LLM as the default chat brain. I will start with `gemma-3-1b-it` and keep the loading logic simple. I can swap to another compact chat model later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IZYNuZyiY5EZ",
      "metadata": {
        "id": "IZYNuZyiY5EZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch, textwrap, json, re\n",
        "\n",
        "# Load google/gemma-3-1b-it using Hugging Face\n",
        "\n",
        "model_id = \"google/gemma-3-1b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "gemma_llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vPU7s0zqeeg_",
      "metadata": {
        "id": "vPU7s0zqeeg_"
      },
      "source": [
        "### Build a routing mechanism to route requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00767550",
      "metadata": {
        "id": "00767550"
      },
      "outputs": [],
      "source": [
        "def generate_media(prompt: str, mode: str):\n",
        "    print(\"mode: \", mode)\n",
        "    # Produce either an image or a short video clip from a text prompt.\n",
        "    if mode == 'image':\n",
        "        print(\"Image Pipeline triggered...\")\n",
        "        return image_pipeline(prompt).images[0]\n",
        "    elif mode == 'video':\n",
        "        print(\"Video Pipeline triggered...\")\n",
        "        return video_pipeline(prompt, num_frames=16, num_inference_steps=20).frames[0]\n",
        "    else:\n",
        "        return 'Invalid mode'\n",
        "\n",
        "def llm_generate(prompt, max_new_tokens=64, temperature=0.7):\n",
        "    # Return a response to the prompt with the loaded gemma\n",
        "    outputs = gemma_llm(prompt, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature)\n",
        "    return outputs[0][0]['generated_text'][-1][\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HsJf8NeWdKIG",
      "metadata": {
        "id": "HsJf8NeWdKIG"
      },
      "outputs": [],
      "source": [
        "def classify_prompt(prompt: str):\n",
        "    \"\"\"Classify the user prompt into QA, image, or video.\"\"\"\n",
        "\n",
        "    # Step 1: Define a system prompt explaining how to classify requests (qa, image, video)\n",
        "    # Step 2: Format the user message and system message as input to the LLM\n",
        "    # Step 3: Generate a response with llm_generate() and parse it using regex\n",
        "    # Step 4: Extract fields \"type\" and \"expanded_prompt\" from the LLM response\n",
        "    # Step 5: Return a dict with classification results or default to {\"type\": \"qa\"} on failure\n",
        "\n",
        "    system = textwrap.dedent(\"\"\"You are a routing assistant for a multimodal generation system.\n",
        "        Decide whether the USER request is:\n",
        "          â€¢ a factual or conversational question  â†’  type = \"qa\"\n",
        "          â€¢ an IMAGE generation request          â†’  type = \"image\"\n",
        "          â€¢ a VIDEO generation request           â†’  type = \"video\"\n",
        "        If it is for image or video, produce an improved, vivid, detailed `expanded_prompt`.\n",
        "        Respond ONLY in this format: {\"type\": \"...\", \"expanded_prompt\": \"...\"}\n",
        "    \"\"\")\n",
        "    messages = [\n",
        "      [\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": [{\"type\": \"text\", \"text\": system},]\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": [{\"type\": \"text\", \"text\": prompt},]\n",
        "          },\n",
        "      ],\n",
        "    ]\n",
        "    response = llm_generate(messages, temperature=0.2)\n",
        "    match = re.search(r'\"type\"\\s*:\\s*\"([^\"]+)\"\\s*,\\s*\"expanded_prompt\"\\s*:\\s*\"([^\"]+)', response)\n",
        "    if match:\n",
        "        try:\n",
        "            result = {\n",
        "              \"type\": match.group(1),\n",
        "              \"expanded_prompt\": match.group(2)\n",
        "            }\n",
        "            return result\n",
        "        except Exception:\n",
        "            pass\n",
        "    # fallback\n",
        "    return {\"type\": \"qa\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LiKCWnVaekar",
      "metadata": {
        "id": "LiKCWnVaekar"
      },
      "source": [
        "### Build the multimodal agent\n",
        "This agent takes a single user prompt, sends it to the `classify_prompt` to determine what kind of task it is, and then calls the appropriate module:\n",
        "- QA: use the chat LLM to generate an answer\n",
        "- Image: use the text-to-image generator\n",
        "- Video: use the text-to-video generator\n",
        "\n",
        "Start with a simple version first. It can be improved later by adding better prompts, guardrails, and citation handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IW9eRebndgY2",
      "metadata": {
        "id": "IW9eRebndgY2"
      },
      "outputs": [],
      "source": [
        "def multimodal_agent(user_prompt: str):\n",
        "    # Step 1: Classify the request\n",
        "    # Step 2: Route the prompt and generate output\n",
        "    decision = classify_prompt(user_prompt)\n",
        "    kind = decision.get('type', 'qa')\n",
        "    print(\"kind: \", kind)\n",
        "    if kind == 'qa':\n",
        "        system = \"You are a helpful assistant.\"\n",
        "        messages = [\n",
        "            [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": [{\"type\": \"text\", \"text\": system},]\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [{\"type\": \"text\", \"text\": user_prompt},]\n",
        "                },\n",
        "            ],\n",
        "        ]\n",
        "        return llm_generate(messages)\n",
        "    else:\n",
        "        return generate_media(decision['expanded_prompt'], mode=kind)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NO3yNkY_eqec",
      "metadata": {
        "id": "NO3yNkY_eqec"
      },
      "source": [
        "### Test the agent\n",
        "Now let's test the multimodal agent end to end. Each prompt will automatically be routed to the correct capability: text Q&A, image generation, or video generation, and display the corresponding output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ff5f1ff",
      "metadata": {
        "id": "8ff5f1ff"
      },
      "outputs": [],
      "source": [
        "from diffusers.utils import export_to_video\n",
        "from IPython.display import display, Video\n",
        "\n",
        "# Step 1: Define a few diverse prompts (QA, image, video)\n",
        "# Step 2: For each prompt, call multimodal_agent and inspect the returned result\n",
        "for p in [\n",
        "    \"What's the capital of Iceland?\",\n",
        "    \"Generate an image of a neon dragon flying over Tokyo at night\",\n",
        "    \"Create a short video of a paper plane folding itself\"\n",
        "]:\n",
        "    result = multimodal_agent(p)\n",
        "    print('\\nPROMPT:', p)\n",
        "    if isinstance(result, str):\n",
        "        print(result)\n",
        "    else:\n",
        "        if hasattr(result, 'save'):\n",
        "            display(result)\n",
        "        else:\n",
        "            vid = export_to_video(result)\n",
        "            print(f\"video path: {vid}\")\n",
        "            display(Video(vid, embed=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a218d0bb",
      "metadata": {
        "id": "a218d0bb"
      },
      "source": [
        "## Interactive Web UI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e16073ff",
      "metadata": {
        "id": "e16073ff"
      },
      "source": [
        "Launch a simple Gradio web interface. We can play with the multimodal agent from the browser.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da883c7",
      "metadata": {
        "id": "4da883c7"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown('# Multimodal Agent')\n",
        "    inp = gr.Textbox(placeholder='Ask or create...')\n",
        "    btn = gr.Button('Submit')\n",
        "    out_text = gr.Markdown()\n",
        "    out_img = gr.Image()\n",
        "    out_vid = gr.Video()\n",
        "\n",
        "    def handle(prompt):\n",
        "        res = multimodal_agent(prompt)\n",
        "        if isinstance(res, str):\n",
        "            return res, None, None\n",
        "        elif hasattr(res, 'save'):\n",
        "            return '', res, None\n",
        "        else:\n",
        "            vid = export_to_video(res)\n",
        "            return '', None, vid\n",
        "\n",
        "    btn.click(handle, inp, [out_text, out_img, out_vid])\n",
        "\n",
        "demo.launch()\n",
        "# demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import os\n",
        "print(psutil.virtual_memory())"
      ],
      "metadata": {
        "id": "NUrs2PPbBBsG"
      },
      "id": "NUrs2PPbBBsG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "37e18ae1",
      "metadata": {
        "id": "37e18ae1"
      },
      "source": [
        "After the UI launches, open the link and generate your own images and videos directly from the browser."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "775d6b75",
      "metadata": {
        "id": "775d6b75"
      },
      "source": [
        "## ðŸŽ‰ Summary\n",
        "\n",
        "* We built a **multi-modal agent** capable of understanding various requests, and routing them to the proper model.\n",
        "\n",
        "Next:\n",
        "* Try experimenting with other T2I and T2V models.\n",
        "* Try making the system more efficient. For example, load a separate lightweight llm for routing, and a more capable llm for QA."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}